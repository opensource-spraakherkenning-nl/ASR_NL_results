<h1>Dutch Open Speech Recognition Benchmark</h1>

Welcome to the benchmark page where researchers and developers report performance of various ASR models on Dutch datasets.

<h2>UT's benchmark</h2>

*UT = University of Twente*

- [Results for N-Best 2008 Dutch Evaluation corpus](./UT/N-Best/nbest_res.md)
- [Results for Jasmin-CGN corpus](./UT/Jasmin/jasmin.md)
- [Results for Common Voice](./UT/CommonVoice/cv.md)
- [Environment setup](./UT/environment.md)
- [Why do the results differ between whisper-timestamped and faster-whisper?](./UT/analysis.md)

The results in **bold** indicate the best performance for the specific subset(s) between all models. The lower, the better.

These results were achieved during the PDI-SSH **O**ral **H**istory - **S**tories at the **M**useum around **Art** ([OH-SMArt](https://www.uva.nl/en/discipline/conservation-and-restoration/research/research-projects/oh-smart/oh-smart.html)) project (2022-2025).

<h2>RU's Kaldi_NL vs. Whisper vs. Wav2vec2.0 evaluation</h2>

*RU = Radboud University*

- [Results on four medical domain datasets](./RU/wer.md)
- [Environment setup](./RU/environment.md)

These results were achieved during the PDI-SSH **Ho**mo **Med**icinalis ([HoMed](https://homed.ruhosting.nl/)) project (2021-2024).

<h2>NISV's Whisper benchmark</h2>

*NISV = Netherlands Institute for Sound & Vision*

- [Results for Broadcast News Speech](./NISV/bn_nl/intro_bn_nl.md)
- [Results for Conversational Telephone Speech](./NISV/cts_nl/intro_cts_nl.md)

The results in **bold** indicate the best performance for the specific subset(s) between all models. The lower, the better.

## Contributions
Feel free to click the link at the top that leads you to the GitHub repository of this website. You may add changes if you want by forking the repository, making changes on your fork, then opening a pull request on the source repository.

## FAQ
**Coming soon**
